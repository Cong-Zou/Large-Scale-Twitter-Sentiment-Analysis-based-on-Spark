#
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

from __future__ import print_function

import sys

from pyspark import SparkContext
from pyspark.streaming import StreamingContext
from pyspark.sql import Row, SparkSession
import pyspark.sql.functions as f
from pyspark.sql.functions import udf
from pyspark.sql.types import *

def getSparkSessionInstance(sparkConf):
	if ('sparkSessionSingletonInstance' not in globals()):
		globals()['sparkSessionSingletonInstance'] = SparkSession\
			.builder\
		.config(conf=sparkConf)\
		.getOrCreate()
	return globals()['sparkSessionSingletonInstance']

afinn_file_name = '/home/ec2-user/spark/AFINN-111.txt'
afinn = dict(map(lambda w_s: (w_s[0], int(w_s[1])), [
			ws.strip().split('\t') for ws in open(afinn_file_name) ]))
#afinn = {"Mohammed" :100, "Sameer":50}


if __name__ == "__main__":
	if len(sys.argv) != 3:
		print("Usage: senti_tweet_app.py <hostname> <port> ", file=sys.stderr)
		sys.exit(-1)
	host, port = sys.argv[1:]
	sc = SparkContext(appName="SentiTweetApp")
	ssc = StreamingContext(sc, 2)

	# Create a socket stream on target ip:port and count the
	# words in input stream of \n delimited text (eg. generated by 'nc')
	lines = ssc.socketTextStream(host, int(port))
	words = lines.flatMap(lambda line: line.split("\n"))
        tweet_count = 0

	print(type(words))

    # Convert RDDs of the words DStream to DataFrame and run SQL query
	def process(time, rdd):
		print("========= %s =========" % str(time))

		try:
			# Get the singleton instance of SparkSession
			spark = getSparkSessionInstance(rdd.context.getConf())

			# Convert RDD[String] to RDD[Row] to DataFrame
			rowRdd = rdd.map(lambda w: Row(word=w))
			wordsDataFrame = spark.createDataFrame(rowRdd)
			my = spark.createDataFrame(rdd.map(lambda w: (w.split('\t'))))
			def my_udf_1(a):
				word_list = a.split()
				freq = []
				for w in word_list:
					freq.append(word_list.count(w))
				score = 0.0
				for elem in list(zip(word_list, freq)):
					score += afinn.get(elem[0],0)*elem[1]
				return score

			def clean_up(a):
				a = str(a)
				a = a.replace(',', ' ')
				return a

			spark.udf.register("my_udf_1",my_udf_1, FloatType())
			my = my.rdd.map(lambda x: (my_udf_1(x[1]), clean_up(x[1]))).toDF(['score','tweet'])
			my.show()
			time_str = str(time)
			time_str = time_str.replace(' ', '_');
			time_str = time_str.replace(':', '_');
			my.write.csv('/home/ec2-user/spark/csv_files/Tweets.'+time_str+'.csv')
			return;
			#my = my.withColumn("x7", my["_2"])
			my_func = udf(lambda x : x.flatMap(lambda x: x).map(lambda z: (z,1).reduceByKey(lambda x,y: x+y)))
			my = my.withColumn('wordcount', my_func(f.col('_2')))
			#dy = my.select('_2').rdd.map(lambda row: tuple(row.split(' '))).map(lambda x: (x,1)).reduceByKey(lambda x,y: x+y)
			#print(dy.take(2))
			my.show()
            
			# Creates a temporary view using the DataFrame.
			wordsDataFrame.createOrReplaceTempView("words")
            
			# Do word count on table using SQL and print it
			wordCountsDataFrame = \
					spark.sql("select word, count(*) as total from words group by word")
			wordCountsDataFrame.show()
		except Exception as e:
			print(e)
			pass
    
        tweet_count = 0
	words.foreachRDD(process)
	ssc.start()
	ssc.awaitTermination()
